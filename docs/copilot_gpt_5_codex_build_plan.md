# Copilot (GPT-5 Codex) — Build Plan

**Filename:** `copilot-gpt5-codex-build-plan.md`

**Purpose:** Detailed build plan, roadmap, architecture, and development checklist for a combined **Learning Platform (coding)**, **Recruitment Platform**, **Assessment Platform**, and **Admin Platform** called **Talent**. This plan is tailored for a stack using **Next.js, Vite, MongoDB, Express** and integrates AI (OpenAI/GPT), **Judge0** for compilation, and **Monaco Editor** across the UI.

---

## 1. High-level goals

1. Single-sign on experience with shared profiles across learning/recruitment/assessment.
2. Auto-generated course content (using OpenAI) broken into levels/prereqs, each with descriptions, YouTube links, Monaco-based code playgrounds and auto-graded coding + MCQ challenges.
3. Assessments with multiple phases (voice/chat transcript, coding, debugging, mcq) with unique questions per assessment.
4. Recruitment experience where developers can be discovered by recruiters, with verification flags for kiosk/onsite assessments.
5. Centralized admin platform to manage technologies, templates, users, content, assessments, leaderboards, analytics.
6. Track user progress, restrict level progression until required challenge is passed.

---

## 2. Guiding principles & constraints

- Use **Monaco Editor** everywhere (single editor choice). Provide IDE-like hints (line highlighting, block hints—not complete solutions).
- Course generation must produce a `language_key` for Judge0 compatibility (map to Judge0 language IDs and compile settings).
- Unique questions generated each time for assessments (prompt-engineering + seeded randomness + question templates).
- No test libraries required for builds; focus on manual testing steps per milestone.
- Security: role-based access, token expiration, rate limits on AI requests.
- Logging & audit for recruitment/assessment events (immutable where needed).

---

## 3. Tech stack

- Frontend: **Next.js** (for SSR pages)(use modern app router), **Vite** for isolated components / admin micro-frontends if required. TypeScript across.
- UI: Tailwind + MUI components (user preferred MUI icons). Monaco Editor (web) for code windows.
- Backend: **Node.js + Express** (TypeScript), microservices-friendly structure .
- Database: **MongoDB** (primary), Redis (optional caching / session / leaderboards).
- AI: **OpenAI GPT-5 Thinking mini** endpoints for generation, plus local prompt orchestration (store prompts/templates in DB).
- Judge: **Judge0** for compile/run/eval. Map `language_key` to Judge0 `language_id` and run params.
- Media: YouTube video linking; support oEmbed fetching for thumbnails.
- Audio: WebRTC + STT (use browser SpeechRecognition or integrate a cloud STT like Whisper/paid API) for voice phase realtime transcript.
- DevOps: Docker, GitHub Actions (or equivalent) for CI/CD, Docker Compose for local dev; Kubernetes for scale optional.

- Use npm install for npm libraries dont directly edit package.json files unless needed

---

## 4. System architecture (textual)

- **Frontend** (NextJS): Authentication pages, Learning app, Recruitment marketplace, Assessment kiosk, Admin dashboard. Realtime features via WebSockets.
- **API Gateway (Express)**: Auth, User management, Course APIs, Assessment APIs, Recruiter APIs, Admin APIs.
- **AI Service**: Middleware that handles OpenAI prompt generation, caching of responses, retry/backoff and prompt templates.
- **Judge0 Adapter**: Service to call Judge0 API, queue submissions, stream results.
- **Media/Storage**: S3-compatible for uploads (resumes, videos, recordings).
- **DB (Mongo)**: Collections for users, profiles, courses, topics, lessons, challenges, assessments, results, technologies, templates.
- **Realtime**: WebSocket server for live assessment, live transcript, editor collaboration/hints.

---

## 5. Data model (collections / key fields)

> Keep names plural (users, profiles, courses).

### users

- `_id`, `email`, `passwordHash`, `roles` (array: `student`, `recruiter`, `admin`, `proctor`), `createdAt`, `lastLogin`, `oauthProviders`.

### profiles

- `userId`, `displayName`, `headline`, `location`, `experienceYears`, `technologies` (array of tech ids), `resumeUrl`, `learningProgress` (map courseId -> progress object), `recruitmentScore`.

### technologies

- `_id`, `name`, `slug`, `judge0_language_key`, `aliases`, `levels` (array), `createdAt`.

### courses

- `_id`, `languageId`, `title`, `language_key`, `levels` (array of {levelName, topics: [topicId]}), `autoGenerated: true|false`, `meta`.

### topics

- `_id`, `courseId`, `title`, `description`, `youtubeLink`, `prerequisites` (array topicIds), `level`, `editorTemplate` (starter code), `challenges`.

### challenges

- `_id`, `topicId`, `type` (`mcq`|`coding`|`debug`), `difficulty`, `judge0Spec`, `hints` (do not contain full solution), `solutionHash` (for grade verification), `randomizeSeed`.

### assessments

- `_id`, `createdBy` (recruiterId), `templateId`?, `phases` (array), `techStack` (array techIds), `duration`, `allowedRetakes`, `uniqueSeed`.

### assessmentResults

- `_id`, `assessmentId`, `candidateId`, `phaseResults` (array), `finalScore`, `verifiedFlag`, `kioskFlag`, `transcriptUrl`.

### adminLogs / audit

- `_id`, `action`, `actorId`, `target`, `timestamp`, `details`.

---

## 6. Important mappings & enumerations

- **Judge0 mapping table**: `technologies.judge0_language_key` -> Judge0 `language_id` and compile flags. (Create mapping CRUD in admin.)
- **Progress states**: `locked`, `unlocked`, `in-progress`, `completed`, `failed`.
- **Roles & permissions**: RBAC mapping in admin.

---

## 7. Key feature flows (detailed)

### 7.1 Course generation flow (AI-driven)

1. Admin/author selects `technology` (or user requests auto-generation).
2. Server calls AI Service with `CourseGeneration` prompt template asking for:
   - All main topics for the language
   - Group by level (Beginner/Intermediate/Advanced)
   - Prerequisites for each topic
   - For each topic: description, 1 short YouTube search query (for link), 1 starter code template, 2 MCQs, 1 coding challenge sketch, `language_key` for Judge0.
3. AI response validated by a lightweight schema and stored as `course`, `topics`, `challenges`.
4. Admin reviews and publishes.

**Important**: Each generated course must include `language_key` and at least one mapping to Judge0.

---

### 7.2 Learning flow for candidate

- Candidate signs up -> auto-profile created.
- Candidate enrolls in a course -> first level unlocked.
- Candidate opens topic: sees description, YouTube link, Monaco editor with starter template.
- Candidate completes interactive steps: watch video, run example code (via Judge0), attempt MCQ.
- Candidate must pass a short `gate challenge` (MCQ + tiny code test) to unlock next topic/level.
- Editor hints: error highlighting, inline hint blocks, non-revealing suggestions. Hints are generated by AI using the failure output from Judge0 and static hint templates. Hint system must be rate-limited.

---

### 7.3 Assessment platform

- Recruiter creates assessment template: chooses technologies (from `technologies`), sets phases & weights, duration.
- System generates unique questions per candidate by using `assessment.template + candidate.seed` and OpenAI to generate variants.

**Phases examples:**

- **Voice Conversation**: AI prompts candidate and evaluates responses. Candidate speaks -> STT transcribes -> transcript saved -> AI grades communication by rubric.
- **Live Debugging**: Candidate sees buggy snippet in Monaco; they must fix/annotate. System tracks edits, time to fix, and hints used.
- **Coding Challenge**: Full coding problem evaluated via Judge0. Score based on correctness + performance + style heuristics (basic static checks).
- **MCQ**: Randomized MCQs.

**Verification / kiosk flag**: If candidate takes test from a registered kiosk/proctored machine, set `kioskFlag=true` and store hardware/proctor token.

---

### 7.4 Recruitment platform

- Recruiters can search developers by `technologies`, `score`, `location`, `availability`, `yearsExperience`.
- Profiles from learning are auto-populated and can be completed by users.
- Recruiters can request custom assessments; results are linked to candidate profiles.
- Verified assessments display `verified` badge.

---

## 8. Authentication & Authorization (priority: implement first everywhere)

**Tasks (auth first):**

1. Implement core auth microservice (Express):
   - Sign up (email/password), email verification
   - Login (JWT access token + refresh token), refresh endpoint
   - OAuth connectors (Google/GitHub/LinkedIn) — optional but recommended for recruiters
   - Password reset flow
   - RBAC middleware to protect APIs
2. Integrate auth into Next.js (SSG/SSR) using `getServerSideProps` where needed and client-side hooks.
3. Create `roles` and `permissions` seed data in DB and admin UI to manage them.

**Security notes:** store refresh tokens in HttpOnly secure cookies. Access token TTL short (e.g., 15m). Implement device/session listing and revoke.

---

## 9. API surface (examples)

> Use RESTful endpoints (or GraphQL later). All endpoints require auth unless public.

### Auth

- `POST /api/auth/signup`
- `POST /api/auth/login` -> returns `{ accessToken, refreshToken }`
- `POST /api/auth/refresh`
- `POST /api/auth/logout`

### Users & Profiles

- `GET /api/users/me`
- `PUT /api/users/me`
- `GET /api/profiles/:id`
- `GET /api/profiles` (search with filters)

### Courses & Topics

- `POST /api/courses/generate` (admin) — triggers AI generated course
- `GET /api/courses`
- `GET /api/courses/:id/topics/:topicId`
- `POST /api/topics/:id/complete` — marks progress & triggers gate validation

### Challenges & Judge0

- `POST /api/judge/submit` -> returns submissionId
- `GET /api/judge/result/:submissionId`
- `POST /api/challenges/:id/attempt` -> logs attempt

### Assessments

- `POST /api/assessments` (recruiter creates)
- `POST /api/assessments/:id/assign` (assign to candidate)
- `POST /api/assessments/:id/start` (start session)
- `POST /api/assessments/:id/submit-phase` (submit phase results)
- `GET /api/assessments/:id/result`

### Admin

- `GET /api/admin/technologies`
- `PUT /api/admin/technologies/:id` (change Judge0 mapping)
- `GET /api/admin/analytics`

---

## 10. Monaco Editor integration (requirements & behavior)

- Use a single shared Monaco config across the app.
- Monaco features:
  - Syntax highlight for `language_key` (map languages as needed)
  - Linting via Judge0 error output (not a static linter) — highlight lines and ranges.
  - Inline hint blocks (not full solutions) — hints stored in `challenges.hints` and enhanced by AI from failure output.
  - Autosave & versioned submissions for assessment audit.
  - Restrict copy/paste during timed assessments (optional).

**Implementation notes:**

- Provide a `monaco-wrapper` React component that accepts `languageKey`, `starterCode`, `readonly`, `onRun`, `onHintRequest`.
- `onRun` calls server which sends code to Judge0 via the Judge0 Adapter.
- For JSX/TSX: pre-configure Babel transform or use Judge0 configs accordingly.

---

## 11. AI & Prompting design

- Store prompt templates in DB with versioning and metadata.
- For course generation: use a template that requests structured JSON output matching our schema.
- For unique questions: templates must accept `seed` and `candidateProfile` to introduce variation.
- Rate-limit AI usage. Cache repeated queries for admin review.
- Always run AI results through a validation layer (JSON Schema) and sanitize output.

**Example prompt sections (appendix contains samples).**

---

## 12. Dev roadmap & milestones (sprints)

> Assume 2-week sprints. Start with auth & infra.

### Sprint 0 — Project setup (1 week)

- Repo scaffolding (monorepo: `apps/frontend-next`, `apps/admin`, `services/api`, `services/ai`, `infra`)
- Docker compose dev setup and MongoDB seed scripts
- Basic CI pipeline skeleton

### Sprint 1 — Auth & Core models (2 weeks)

- Implement Auth microservice (signup/login/refresh)
- Seed roles & technologies
- Implement `users` and `profiles` models + basic CRUD
- Setup Next.js auth integration

### Sprint 2 — Course model + Monaco skeleton (2 weeks)

- Create `courses`, `topics`, `challenges` models
- Implement `monaco-wrapper` component and sample page
- Basic Judge0 adapter and `POST /api/judge/submit` stub

### Sprint 3 — AI course generation + admin UI (2 weeks)

- AI Service + `POST /api/courses/generate`
- Admin UI for review/publish of generated courses
- Validation + storage of generated content

### Sprint 4 — Learning flows + gating (2 weeks)

- Topic view, gate challenge flow, progress tracking
- Editor hints pipeline and hint UI
- Leaderboard basic implementation

### Sprint 5 — Assessment builder & execution (2 weeks)

- Assessment templates & unique question generator
- Voice phase prototype (web speech to text), transcript saving
- Coding phase with Judge0 grading and attempt logging

### Sprint 6 — Recruitment marketplace (2 weeks)

- Search/filter APIs for recruiters
- Profile enrichment, verified assessment flag
- Recruiter dashboard basic

### Sprint 7 — Analytics, audit, deployment prep (2 weeks)

- Admin analytics, logs, audit trail
- Add S3 uploads, transcript storage
- CI/CD finalization and infra hardening

### Sprint 8 — Polish & scale (2 weeks)

- Rate limiting, monitoring, proctoring/kiosk integration
- Performance tuning and final UX polish

---

## 13. Developer documentation & checkpoints (what to test each phase)

- **Phase 1 (Auth)**: Test signup/login/oauth. Verify token expiry and refresh. Test role gating for a protected route.
- **Phase 2 (Monaco + Judge0)**: Run sample code through editor -> judge -> get result and line highlights.
- **Phase 3 (AI generation)**: Trigger course generation, verify schema, edit/publish.
- **Phase 4 (Gate challenge)**: Complete gate and verify progress transition.
- **Phase 5 (Assessment)**: Run full assessment for a dummy candidate, generate transcript, verify scoring and `kioskFlag` behavior.

---

## 14. Unique constraints & recommendations

- **Question uniqueness**: Use `seed = hash(candidateId + assessmentId + timestamp)` and pass to AI to generate variants.
- **Plagiarism**: Store hash of solutions; flag identical submissions for manual review.
- **Judge0 limits**: Batch / queue submissions; use worker pool and backoff.
- **Hints**: Limit hints per challenge and record hint usage as penalty.
- **Accessibility**: Make sure editor is keyboard accessible and transcripts have captions.

---

## 15. Security & Compliance

- Encrypt PII at rest where needed.
- GDPR/Privacy: provide data export for users and deletion endpoints.
- Rate limit AI endpoints per user and per IP.
- Penetration test checklist before production.

---

## 16. Monitoring & observability

- Collect metrics: API latency, judge queue length, AI call count, assessment throughput, error rates.
- Alerts: Judge backlog > X, AI errors > Y, auth failures spike.
- User analytics: progress funnel, dropout rates per topic, avg attempts per challenge.

---

## 17. Appendix (snippets & templates)

### 17.1 Sample course-generation prompt (simplified)

```
You are a course-author assistant. Produce a JSON array of topics for the language: <LANGUAGE_NAME>.
Each topic object: { "title", "level": "Beginner|Intermediate|Advanced", "description", "youtubeSearchQuery", "starterCode", "prerequisites": [], "mcqs": [{"q","options","answerIndex"}], "codingSketch": {"taskDesc","sampleIO"}, "language_key" }
Return only valid JSON.
```

### 17.2 Judge0 mapping example

| technology | language_key | judge0_language_id         |
| ---------- | ------------ | -------------------------- |
| javascript | `javascript` | 63                         |
| python     | `python3`    | 71                         |
| jsx        | `jsx`        | (preprocess to JS + Babel) |

(Real mapping will be stored in `technologies` collection.)

---

## 18. Final notes & next steps for you

1. I prioritized **auth** first (you asked to test early). Start by creating the monorepo and implementing the auth microservice + seed data.
2. I'll include more granular prompts, sample API contracts and sample UI components if you want — say the word and I will extend.
3. If you'd like, I can also produce a one-page ERD or a minimal Postman collection next.

---
Add a progress doc too 
_End of build plan._
